{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec0'></a>\n",
    "# Predictive annotation of quantification methods with Noun Chunks\n",
    "Idea is to use the descriptive text for each dataset to predict which quantification method was used for the experiments for the dataset. Below is an outline for the process.<br>\n",
    "- <a href='#sec1'><b>Data preparation</b></a>\n",
    "- <a href='#sec2'><b>NLP</b></a>\n",
    "- <a href='#sec3'><b>Classification (Initial)</b></a>\n",
    "- <a href='#sec4'><b>Classification (Deeper)</b></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from gensim import matutils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score,\n",
    "        average_precision_score, f1_score,\n",
    "        brier_score_loss, classification_report,\n",
    "        precision_recall_curve, roc_auc_score, roc_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1-1'></a>\n",
    "#### 1. Read in data and process df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(125116 unique tokens: [') luminal a tumors', '1 tumors', '24 lymph node', '24 lymph node negative grade 1 luminal a primary breast tumors', '4405 proteins']...)\n"
     ]
    }
   ],
   "source": [
    "with open('nlp15_data/dfs/all_fields_nchunks_df_quant_dummies.pickle', 'rb') as infile_df:\n",
    "    df = pickle.load(infile_df)\n",
    "\n",
    "protocols_tfidf = corpora.MmCorpus('nlp15_data/bow_and_tfidf/protocols_tfidf_nchunks.mm')\n",
    "whole_tfidf = corpora.MmCorpus('nlp15_data/bow_and_tfidf/whole_tfidf_nchunks.mm')\n",
    "\n",
    "my_dictionary = corpora.Dictionary.load('nlp15_data/whole_dictionary_nchunks.dict')\n",
    "print(my_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>sample_protocol</th>\n",
       "      <th>data_protocol</th>\n",
       "      <th>description</th>\n",
       "      <th>instruments</th>\n",
       "      <th>exp_types</th>\n",
       "      <th>quant_methods</th>\n",
       "      <th>labhead_fullname</th>\n",
       "      <th>silac</th>\n",
       "      <th>ms1_label_free</th>\n",
       "      <th>spectrum_counting</th>\n",
       "      <th>tmt</th>\n",
       "      <th>itraq</th>\n",
       "      <th>label_free</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PXD000029</td>\n",
       "      <td>[breast cancer tissue lysates, reduction, alky...</td>\n",
       "      <td>[proteomics data, proteome discoverer, fdr&lt;0.0...</td>\n",
       "      <td>[current prognostic factors, precise risk-disc...</td>\n",
       "      <td>LTQ Orbitrap Velos</td>\n",
       "      <td>Shotgun proteomics</td>\n",
       "      <td>itraq</td>\n",
       "      <td>Pavel Bouchal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PXD000164</td>\n",
       "      <td>[protein extraction, catheter biofilm small pi...</td>\n",
       "      <td>[tryptic digest, reversed phase, rp) chromatog...</td>\n",
       "      <td>[term-catheterization, catheter-associated bac...</td>\n",
       "      <td>LTQ Orbitrap Velos</td>\n",
       "      <td>Shotgun proteomics</td>\n",
       "      <td>label free</td>\n",
       "      <td>Katharina Riedel</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_id                                    sample_protocol  \\\n",
       "0  PXD000029  [breast cancer tissue lysates, reduction, alky...   \n",
       "1  PXD000164  [protein extraction, catheter biofilm small pi...   \n",
       "\n",
       "                                       data_protocol  \\\n",
       "0  [proteomics data, proteome discoverer, fdr<0.0...   \n",
       "1  [tryptic digest, reversed phase, rp) chromatog...   \n",
       "\n",
       "                                         description         instruments  \\\n",
       "0  [current prognostic factors, precise risk-disc...  LTQ Orbitrap Velos   \n",
       "1  [term-catheterization, catheter-associated bac...  LTQ Orbitrap Velos   \n",
       "\n",
       "            exp_types quant_methods  labhead_fullname  silac  ms1_label_free  \\\n",
       "0  Shotgun proteomics         itraq     Pavel Bouchal      0               0   \n",
       "1  Shotgun proteomics    label free  Katharina Riedel      0               0   \n",
       "\n",
       "   spectrum_counting  tmt  itraq  label_free  \n",
       "0                  0    0      1           0  \n",
       "1                  0    0      0           1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2387, 2387, 2387)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(protocols_tfidf), len(whole_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>\n",
    "## Classification (initial)\n",
    "<a href='#sec0'>(Back to top)</a><br><br>\n",
    "<i>Here, I'll focus on binary classification for each category. For example, one task would be to distinguish 'silac' from everything else. Ideally, it should be multi-class classification, but since a dataset can contain multiple quantification methods, for now, I'll stick with yes/no type binary classification for each quantification method that I picked. Here, I'll use Tf-Idf vectors only and take a quick look at the performance of different classifiersvisually with ROC curves. </i><br>\n",
    "<br>\n",
    "Classifiers to try: MultinomialNB, Logistic Regression, Random Forest (and SVM maybe?)<br>\n",
    "<br>\n",
    "For each classifier for each text set for each type of word vectors for each quantification type:\n",
    "1. Split into train/test with a fixed random_state for comparison\n",
    "2. Run vanilla classifier without much hyperparameter tuned and plot ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    my_dictionary = corpora.Dictionary.load('nlp13_data/whole_dictionary.dict')z\n",
    "    print(my_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(corpus, target, num_terms=22736, test_size=0.2, random_state=777):\n",
    "    # Set features and y\n",
    "    x = matutils.corpus2dense(corpus, num_terms=num_terms).T\n",
    "    y = np.array(target)\n",
    "    \n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Classifiers to test\n",
    "    classifiers = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(n_jobs=8)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        result = {}\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_proba = clf.predict_proba(X_test)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])\n",
    "        result['report'] = report\n",
    "        result['fpr'] = fpr\n",
    "        result['tpr'] = tpr\n",
    "        results[name] = result\n",
    "    return results\n",
    "\n",
    "\n",
    "def multiple_classify(my_corpora, target, target_labe=None, num_terms=22736, test_size=0.2, random_state=777):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    t = np.linspace(0, 1, 10)\n",
    "    ax_pos = 1\n",
    "\n",
    "    for corpus_name, corpus in my_corpora.items():\n",
    "        results = classify(corpus, target, num_terms=22736)\n",
    "\n",
    "        ax = fig.add_subplot(2, 3, ax_pos)\n",
    "        ax.plot(t, t, ls='--', lw=0.5, color='0.4')\n",
    "        for k, v in results.items():\n",
    "            #print('='*5, 'Report for %s' % k, '='*10)\n",
    "            #print(v['report'])\n",
    "            fpr = v['fpr']\n",
    "            tpr = v['tpr']\n",
    "            ax.plot(fpr, tpr, label=k)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('False Positive Rate')#, fontsize=14)\n",
    "        ax.set_ylabel('True Positive Rate')#, fontsize=14)\n",
    "        a = ax.set_title('Annotating \\'%s\\'\\nby %s' % (target_labe, corpus_name))#, fontsize=14)\n",
    "        ax_pos += 1\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score,\n",
    "        average_precision_score, f1_score,\n",
    "        brier_score_loss, classification_report,\n",
    "        precision_recall_curve, roc_auc_score, roc_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpora = {\n",
    "    'sample_protocol': sample_tfidf,\n",
    "    'data_protocol': data_tfidf,\n",
    "    'both_protocols': protocols_tfidf,\n",
    "    'description': description_tfidf,\n",
    "    'all_combined': whole_tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_classify(my_corpora, df.silac, target_labe='silac', num_terms=22736, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_classify(my_corpora, df.ms1_label_free, target_labe='ms1_label_free', num_terms=22736, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_classify(my_corpora, df.spectrum_counting, target_labe='spectrum_counting', num_terms=22736, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_classify(my_corpora, df.tmt, target_labe='tmt', num_terms=22736, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_classify(my_corpora, df.itraq, target_labe='itraq', num_terms=22736, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_classify(my_corpora, df.label_free, target_labe='label_free', num_terms=22736, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>They actually look very good though the prediction process is quite quick and dirty! It appears combination of sample- and data-processing protocols, as well as its combination with the description field, perform well. As for the classifiers, Naive Bayes performed quite poorly, while Logistic Regression performed best. I can focus on LR and do proper CV and assessements next. Here, all the words were used (i.e. num_terms=(number of unique tokens in the dictionary)), however, that is quite likely unnecessary so columns should be systematically eliminated. Hyperparameters will be tuned too.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>\n",
    "## Classification (deeper dig)\n",
    "<a href='#sec0'>(Back to top)</a><br><br>\n",
    "Looks like automatic annotation is quite possible for most quantification methods! 'Description' appear not so useful as the protocol fields. Here I'll focus on both_protocols corpora (and maybe whole_corpus too) and assess the classification more properly.<br>\n",
    "<br>\n",
    "Classifiers to use: Logistic Regression and Ramdom Forest<br>\n",
    "Class label: Only try on 'silac' here\n",
    "<br>\n",
    "For each classifier for each text set for each type of word vectors for each quantification type:<br>\n",
    "1. Put features and target labels in 4~5 CV fold\n",
    "2. Run vanilla classifier without much hyperparameter tuned and collect:\n",
    "    - accuracy\n",
    "    - precision, recall, & F1\n",
    "    - precision_recall curve\n",
    "    - average_precision_score\n",
    "    - fpr, tpr, roc_curve, roc_auc\n",
    "    - brier loss\n",
    "3. Tabulate and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyCV(clf, corpus, target, name=None, n_splits=5, num_terms=22736):\n",
    "    # Set features and y\n",
    "    if type(corpus) == np.ndarray:    # This is just for convenience later. Not a good coding design though.\n",
    "        X = corpus\n",
    "    else:\n",
    "        X = matutils.corpus2dense(corpus, num_terms=num_terms).T\n",
    "    y = np.array(target)\n",
    "    \n",
    "    # Cross Validation Loop\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    results = []\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        \n",
    "        # create result dictionary\n",
    "        result = {}\n",
    "        result['classifier'] = name\n",
    "\n",
    "        # Fit a model and predict\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_proba = clf.predict_proba(X_test)\n",
    "        \n",
    "        # Get metrics\n",
    "        result['CV_fold'] = n_splits\n",
    "        result['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "        result['precision'] = precision_score(y_test, y_pred)\n",
    "        result['recall'] = recall_score(y_test, y_pred)\n",
    "        result['average_precision'] = average_precision_score(y_test, y_proba[:, 1])\n",
    "        result['Brier_loss'] = brier_score_loss(y_test, y_proba[:, 1])\n",
    "        result['f1'] = f1_score(y_test, y_pred)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])\n",
    "        result['fpr'] = fpr\n",
    "        result['tpr'] = tpr\n",
    "        result['roc_auc'] = roc_auc_score(y_test, y_proba[:, 1])\n",
    "        \n",
    "        precision_, recall_, _ = precision_recall_curve(y_test, y_proba[:, 1])\n",
    "        result['precision_vals'] = precision_\n",
    "        result['recall_vals'] = recall_\n",
    "\n",
    "        results.append(result)\n",
    "            \n",
    "    results = pd.DataFrame(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_stats(results, label):\n",
    "    col = results[label]\n",
    "    mean = col.mean()\n",
    "    std = col.std()\n",
    "    print('%s: %.2f +/- %.3f' % (label, mean, std))\n",
    "    return\n",
    "\n",
    "def summarize_results(results, clf_label='', target_label='', plot_result=True):\n",
    "    score_cols = [\n",
    "        'accuracy',\n",
    "        'precision',\n",
    "        'recall',\n",
    "        'f1',\n",
    "        'average_precision',\n",
    "        'roc_auc',\n",
    "        'Brier_loss',\n",
    "    ]\n",
    "    print('='*10, '%s %d-fold CV stats' % (clf_label, len(results)), '='*10)\n",
    "    for col in score_cols:\n",
    "        get_stats(results, col)\n",
    "    \n",
    "    if plot_result:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        for i in range(len(results)):\n",
    "            fpr = results.fpr.iloc[i]\n",
    "            tpr = results.tpr.iloc[i]\n",
    "            ax1.plot(fpr, tpr, label=('CV %d' % (i+1)))\n",
    "            ax1.set_xlabel('False Positive Rate')\n",
    "            ax1.set_ylabel('True Positive Rate')\n",
    "            ax1.set_title('ROC: Logistic Regr. (\\'%s\\')' % target_label)\n",
    "            ax1.legend()\n",
    "\n",
    "            precision_vals = results.precision_vals.iloc[i]\n",
    "            recall_vals = results.recall_vals.iloc[i]\n",
    "            ax2.plot(recall_vals, precision_vals, label=('CV %d' % (i+1)))\n",
    "            ax2.set_xlabel('Recall')\n",
    "            ax2.set_ylabel('Precision')\n",
    "            ax2.set_title('Precision-Recall: Logistic Regr. (\\'%s\\')' % target_label)\n",
    "            ax2.legend()\n",
    "\n",
    "        fig.tight_layout()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silac_results = classifyCV(LogisticRegression(), protocols_tfidf, df.silac,\n",
    "    name='Logistic Regression', n_splits=10, num_terms=22736)\n",
    "summarize_results(silac_results, clf_label='Logistic Regression', target_label='silac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_results = classifyCV(LogisticRegression(), whole_tfidf,  df.silac,\n",
    "                                name='Logistic Regression', n_splits=10, num_terms=22736)\n",
    "summarize_results(whole_results, clf_label='Logistic Regression', target_label='silac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Try L1 regularization instead of L2 (which is default)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(penalty='l1')\n",
    "silac_results = classifyCV(clf1, protocols_tfidf, df.silac,\n",
    "    name='Logistic Regression', n_splits=10, num_terms=22736)\n",
    "summarize_results(silac_results, clf_label='Logistic Regression', target_label='silac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Decrease regularization strength (C value)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression(penalty='l1', C=1000)\n",
    "silac_results = classifyCV(clf1, protocols_tfidf, df.silac,\n",
    "    name='Logistic Regression', n_splits=10, num_terms=22736)\n",
    "summarize_results(silac_results, clf_label='Logistic Regression', target_label='silac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>No fitting of intercept</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression(penalty='l1', fit_intercept=False)\n",
    "silac_results = classifyCV(clf1, protocols_tfidf, df.silac,\n",
    "    name='Logistic Regression', n_splits=10, num_terms=22736)\n",
    "summarize_results(silac_results, clf_label='Logistic Regression', target_label='silac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>L1 seems to eliminate some unnecessary features and improve the recall. Try removing columns first by Random Forest and the try again with smaller feature</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=10, n_estimators=100, n_jobs=8)\n",
    "silac_results = classifyCV(rfc, protocols_tfidf, df.silac,\n",
    "    name='Logistic Regression', n_splits=5, num_terms=22736)\n",
    "summarize_results(silac_results, clf_label='Logistic Regression', target_label='silac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of non-zero features\n",
    "rfc.n_features_, np.count_nonzero(rfc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rfc.feature_importances_).describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.hist(rfc.feature_importances_, bins=500)\n",
    "plt.ylim([0, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = matutils.corpus2dense(protocols_tfidf, num_terms=22736).T\n",
    "\n",
    "new_inds1 = np.argwhere(rfc.feature_importances_ >= 0.001).flatten()\n",
    "new_inds2 = np.argwhere(rfc.feature_importances_ >= 0.005).flatten()\n",
    "new_inds3 = np.argwhere(rfc.feature_importances_ >= 0.01).flatten()\n",
    "\n",
    "X1 = X0[:, new_inds1]\n",
    "X2 = X0[:, new_inds2]\n",
    "X3 = X0[:, new_inds3]\n",
    "\n",
    "print('Shape before:', X0.shape)\n",
    "print('Shape X1   :', X1.shape)\n",
    "print('Shape X2   :', X2.shape)\n",
    "print('Shape X3   :', X3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fraction of silac class\n",
    "print('Fraction with silac = %.2f%%' % (100*df.silac.sum()/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcX1 = RandomForestClassifier(max_depth=5, n_estimators=10, n_jobs=8)\n",
    "rfcX2 = RandomForestClassifier(max_depth=5, n_estimators=10, n_jobs=8)\n",
    "rfcX3 = RandomForestClassifier(max_depth=5, n_estimators=10, n_jobs=8)\n",
    "\n",
    "res_X1 = classifyCV(rfcX1, X1, df.silac, name='RFC', n_splits=10, num_terms=22736)\n",
    "res_X2 = classifyCV(rfcX2, X2, df.silac, name='RFC', n_splits=10, num_terms=22736)\n",
    "res_X3 = classifyCV(rfcX3, X3, df.silac, name='RFC', n_splits=10, num_terms=22736)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results(res_X1, clf_label='RFC with X1', target_label='silac', plot_result=True)\n",
    "summarize_results(res_X2, clf_label='RFC with X2', target_label='silac', plot_result=True)\n",
    "summarize_results(res_X3, clf_label='RFC with X3', target_label='silac', plot_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrX1 = LogisticRegression(penalty='l1', C=1)\n",
    "lrX2 = LogisticRegression(penalty='l1', C=1)\n",
    "lrX3 = LogisticRegression(penalty='l1', C=1)\n",
    "\n",
    "res_X1lr = classifyCV(lrX1, X1, df.silac, name='LR', n_splits=10, num_terms=22736)\n",
    "res_X2lr = classifyCV(lrX2, X2, df.silac, name='LR', n_splits=10, num_terms=22736)\n",
    "res_X3lr = classifyCV(lrX3, X3, df.silac, name='LR', n_splits=10, num_terms=22736)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results(res_X1lr, clf_label='LR with X1', target_label='silac', plot_result=True)\n",
    "summarize_results(res_X2lr, clf_label='LR with X2', target_label='silac', plot_result=True)\n",
    "summarize_results(res_X3lr, clf_label='LR with X3', target_label='silac', plot_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_dict = {v:k for k, v in my_dictionary.token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_collection = [new_inds2, new_inds3]\n",
    "for i, ind_set in enumerate(inds_collection):\n",
    "    print('='*10, 'Feature Set %d' % (i+2),'='*10)\n",
    "    for ind in ind_set:\n",
    "        print(reverse_dict[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBVIOUSLY!!\n",
    "Looks like RFC with Feature Set 3 is sufficient and good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
